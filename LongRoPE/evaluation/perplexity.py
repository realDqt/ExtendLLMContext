# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.
import os
import gc
import sys
import logging
import argparse
import warnings

import torch
import datasets
import transformers
from tqdm import tqdm

sys.path.append(os.path.join(os.path.split(__file__)[0], os.path.pardir))
from rope import load_model
from utils.save_memory import replace_methods


logger = logging.getLogger(__file__)


def compute_perplexity(
    dataset: datasets.IterableDatasetDict,
    model: transformers.AutoModelForCausalLM,
    tokenizer: transformers.AutoTokenizer,
    add_start_token: bool = True,
    num_tokens: int = None,
    sliding_window: int = 256,
    max_sliding_count: int = 10,
    truncate: bool = False,
    use_cache: bool = False,
    save_memory: bool = False,
    device: torch.device = None,
):
    r"""Compute "sliding window" perplexity on a dataset. Validated against the calculations reported in arXiv 2306.15595  """

    if save_memory:
        replace_methods(model)

    if add_start_token:
        # leave room for <BOS> token to be added:
        assert tokenizer.bos_token is not None, \
            "Input model must already have a BOS token if using add_start_token=True. " \
            "Please use a different model, or set add_start_token=False"
        max_tokenized_len = num_tokens - 1
    else:
        max_tokenized_len = num_tokens

    encoded_texts = dataset["input_ids"]
    attn_masks = dataset["attention_mask"]

    if num_tokens and truncate:
        encoded_texts = [x[:max_tokenized_len] for x in encoded_texts]
        attn_masks = [x[:max_tokenized_len] for x in attn_masks]
        sliding_window = max_tokenized_len

    pbar = tqdm(total=len(encoded_texts), disable=logger.level <= logging.INFO)

    nlls = []
    for encoded_text in encoded_texts:

        labels = torch.tensor([encoded_text], device=device)
        seq_len = labels.size(1)
        seq_len = min(seq_len, max_sliding_count * sliding_window)

        prev_end_loc = 0

        for begin_loc in range(0, seq_len, sliding_window):

            end_loc = min(begin_loc + max_tokenized_len, seq_len)
            trg_len = end_loc - prev_end_loc
            input_ids = labels[:, begin_loc:end_loc]

            if add_start_token:
                bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * input_ids.size(0), device=device)
                input_ids = torch.cat([bos_tokens_tensor, input_ids], dim=1)

            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                with torch.cuda.device(device):
                    outputs = model(input_ids, labels=target_ids, use_cache=use_cache)
                    neg_log_likelihood = outputs.loss
                    outputs = None
                    input_ids = None
                    target_ids = None
                    gc.collect()
                    torch.cuda.empty_cache()
                    
            # with torch.no_grad():
            #     # 1) forward 得到 logits
            #     outputs = model(input_ids, use_cache=use_cache)
            #     logits = outputs.logits            # [B, L, V]

            #     # 2) shift 对齐到下一个 token
            #     shift_logits = logits[..., :-1, :].contiguous()    # [B, L-1, V]
            #     shift_labels = target_ids[..., 1:].contiguous()   # [B, L-1]

            #     # 3) 只保留本次 window 的 trg_len 部分
            #     shift_logits = shift_logits[:, -trg_len:, :]       # [B, trg_len, V]
            #     shift_labels = shift_labels[:, -trg_len:]          # [B, trg_len]

            #     # 4) flatten 并计算 loss，忽略 -100
            #     loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            #     neg_log_likelihood = loss_fct(
            #         shift_logits.view(-1, shift_logits.size(-1)), # [B*trg_len, V]
            #         shift_labels.view(-1)                         # [B*trg_len]
            #     )

            #     # 清理显存
            #     outputs = None
            #     logits = None
            #     shift_logits = None
            #     shift_labels = None
            #     gc.collect()
            #     torch.cuda.empty_cache()


            nlls.append(neg_log_likelihood)

            ppl = float(torch.exp(torch.stack(nlls).mean()).float().cpu())
            pbar.set_postfix(ppl=ppl)

            prev_end_loc = end_loc
            if end_loc == seq_len:
                break

        pbar.update(1)

    return float(torch.exp(torch.stack(nlls).mean()).float().cpu())


def main(args):
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    logger.info(f"Loading tokenized dataset: {args.tokenized}")
    dataset = datasets.load_from_disk(args.tokenized)
    if args.dataset_min_tokens:
        dataset = dataset.filter(lambda x: x["tokenized_len"] >= args.dataset_min_tokens, num_proc=args.num_proc)
    if args.samples:
        dataset = dataset[:args.samples]

    logger.info(f"Loading model: {args.model}")
    rope_method = os.environ.get('ROPE_METHOD', None)
    if rope_method.startswith('longrope'):
        rope_params = {
            'longrope_params_path': os.environ['LONGROPE_RESCALE_FACTOR'],
            'longrope_scaling_policy': os.environ['LONGROPE_SCALING_POLICY'],
        }
    else:
        rope_params = None

    if args.dtype is None:
        dtype = 'auto'
    else:
        dtype = getattr(torch, args.dtype)
        torch.set_default_dtype(dtype)

    max_num_tokens = int(os.environ['TARGET_LENGTH'])
    logger.info(f"Loading model: {args.model} (max_position_embeddings={max_num_tokens})")
    model = load_model(
        model_name_or_path=args.model,
        rope_method=rope_method,
        max_position_embeddings=max_num_tokens,
        rope_params=rope_params,
        attn_implementation=args.attn_implementation,
        attn_sliding_window=args.attn_sliding_window,
        torch_dtype=dtype,
        save_memory=args.save_memory,
        device_map='auto',
    )

    logger.info(f"Model vocab size {model.config.vocab_size} = tokenizer vocab size {len(tokenizer)}")
    if model.config.vocab_size != len(tokenizer):
        logger.info(f"Model vocab size {model.config.vocab_size} != tokenizer vocab size {len(tokenizer)}")
        logger.info(f"Resizing model vocab size to {len(tokenizer)}")
        model.resize_token_embeddings(len(tokenizer))
    logger.info(f"Begin Test")
    results = []
    for num_tokens in map(int, args.num_tokens.split(',')):
        ppl = compute_perplexity(
            model=model,
            tokenizer=tokenizer,
            dataset=dataset,
            add_start_token=tokenizer.bos_token is not None,
            num_tokens=num_tokens,
            sliding_window=args.ppl_sliding_window,
            truncate=args.truncate,
            use_cache=args.use_cache,
            device='cuda',
        )
        logger.info(f"Perplexity (num_tokens = {num_tokens}) = {ppl}")
        results.append([num_tokens, ppl])

    if args.output_file:
        with open(args.output_file, "a", encoding="utf-8") as f:
            f.write('length,ppl\n')
            f.write('\n'.join([','.join(map(str, result)) for result in results]))
            f.write('\n')


if __name__ == "__main__":
    warnings.simplefilter("ignore")
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(message)s',
        datefmt='%m-%d %H:%M',
    )

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str)
    parser.add_argument("--tokenized", type=str)
    parser.add_argument("--dataset-min-tokens", type=int, default=None)
    parser.add_argument("--samples", type=int, default=None)
    parser.add_argument("--num-tokens", type=str, default='4096,8192,16384')
    parser.add_argument("--ppl-sliding-window", type=int, default=256)
    parser.add_argument("--truncate", action="store_true")
    parser.add_argument("--output-file", type=str, default=None)
    parser.add_argument("--attn-implementation", type=str, default="eager")
    parser.add_argument("--attn-sliding-window", type=int, default=-1)
    parser.add_argument("--use-cache", action="store_true")
    parser.add_argument("--num-proc", type=int, default=4)
    parser.add_argument("--save-memory", action="store_true")
    parser.add_argument("--dtype", type=str, default=None)
    main(parser.parse_args())
